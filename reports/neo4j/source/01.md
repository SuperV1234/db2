

## Query benchmarker

The second Python script, which runs the queries, benchmarks them and produces the graphs thanks to **matplotlib**, has a very straightforward implementation:

```python
# Create a `master` 
m = master(make_connection("neo4j", "admin"))

# Benchmark queries
bench_query('query0', '''
    MATCH (n:patient)
    RETURN n''')

bench_query('query1', '''
    MATCH (n:patient)
    WHERE n.n = "SIVV33W0"
    RETURN n''')

bench_query('query2', '''
    MATCH (p:patient)-[r:measure]->(m:measurement)
    WITH p, m, count(m) as relcount
    WHERE p.lwalk_td < 5000 AND p.w <> 5000 AND relcount > 4
    RETURN p''')
```

The `bench_query` functions is implemented as follows:

```python
# Executes `q`, timing it and outputting results
def bench_query(lbl, q):
    # Perform queries and time them
    # Write results as newline-separated values
    for i in range(0, 30):
        start_timer()
        m.exec_query(q)
        print(end_timer())
```

The `start_timer` and `end_timer` functions make use of the `time.perf_counter()` Python high-precision timer in order to retrieve the execution time of every single query:

```python
# Benchmark utilities
t0 = []
def start_timer():
    global t0
    t0.append(time.perf_counter())

def end_timer():
    global t0
    val = time.perf_counter() - t0.pop()
    return val
```

## Plotting script

After having generated all dataset results in text files, where every query iteration execution time is written to a different line, the plotting script will take care of reading the files and producing histogram plots.

Given a list of dataset output paths `datasets` and a count of measurements per output, the `create_plot` is called for every query:

```python
create_plot("query 0", datasets, 0, count, "plots/query0.png")
create_plot("query 1", datasets, 30, count, "plots/query1.png")
create_plot("query 2", datasets, 60, count, "plots/query2.png")
```

Its implementation is as follows:

```python
def create_plot(plot_title, datasets, offset, count, output_path):
    # Iterate over the dataset benchmark outputs
    for dataset_path in datasets:
        # Get statistical values
        first,avg,conf = statistics(dataset_path, offset, count)

        # Create two bars using pyplot
        b_first = plt.bar(x, first, 0.5, color='b')
        b_avg = plt.bar(x, avg, 0.5, color='r')
    
    # Plot to file
    plt.savefig(output_path)
    plt.clf()
```

Statistics are calculated with this function:

```python
def statistics(dataset_path, offset, count):
    with open(dataset_path, 'r') as f:
        values = [float(x) for x in f.readlines()]

    # Get first query time
    first = values[offset]

    # Other values
    other_values = values[offset+1:offset+count]

    # Get average time of remaining queries
    mean = numpy.mean(other_values)
    stddev = numpy.std(other_values)
    conf = 0.95 * (stddev / math.sqrt(len(other_values)))

    return (first, mean, conf)
```

The `first_and_avg` function simply loads the dataset timing results in memory and returns a tuple containing the first query time and the average time of the remaining queries.

## Automation script

In order to automate the whole benchmarking process, a simple **bash** script was implemented to load all datasets and execute the queries on them:

```bash
# Create `results` folder if required
mkdir -p results

# Dataset N array
VALUES=(10 100 1000 10000 100000)

# Load dataset chunk N array
CHUNKS=(1 1 1 5 10)

# Next chunk value index
ICHUNK=0

for i in "${VALUES[@]}"
do
    # Dataset path
    DS="../../dataset_lokomat/output/ds${i}.json"

    # Output graph path
    OF="./results/r${i}.png"

    # Load dataset
    python3 -O ./load_dataset.py "${DS}" "${CHUNKS[$ICHUNK]}"

    # Increment index for next chunk
    ((ICHUNK++))

    # Run queries and create plots
    python3 -O ./queries.py "${OF}"
done

# Create plots
python3 -O ./make_plots.py
```

The script, in short, simply runs the previously described Python scripts for every randomly-generated dataset, automatically passing the dataset path and a reasonable chunk value in every execution. When the datasets have been processed, the `make_plots` script is finally called to produce the histogram images.


# Experiments

Experiments were ran using the automation script shown in the chapter above. Every experiment executed the following steps:

1. Completely clear the Neo4j instance.

2. Load the dataset into the current Neo4j instance, with the `load_dataset.py` script.

3. Execute queries and output all measuraments in the corresponding text file, with the `queries.py` script.

4. Read measurements from the created output file and plot histogram graphs, with the `make_plots.py` script.

The results of the queries are provided as histograms. Every plot image represents a single query, over all datasets. Two bars are plotted per dataset size: the first bar represents the **execution time of the first query**, the second bar represents the **average execution time of the remaining queries**.

The **X axis** represents the size of the datasets.

The **Y axis** represents the execution time, in milliseconds.

In addition, a spreadsheet containing a line chart and statistical measurements for every query was created.

## Result graphs

\begin{figure}[H]
\centering
\includegraphics{source/query0.png}
\caption{Benchmark results: query 0}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{source/query1.png}
\caption{Benchmark results: query 1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{source/query2.png}
\caption{Benchmark results: query 2}
\end{figure}

## Result spreadsheet

The spreadsheet is available on [the GitHub repository](https://github.com/SuperV1234/db2). Every sheet of the file represents a single dataset and contains the following elements:

* All measurements for the three queries.

* The average time.

* The standard deviation of the queries' time.

* The confidence interval, calculated using the standard deviation and the $0.95$ confidence coefficient.

An example screenshot of the spreadsheet is shown below:


\begin{figure}[H]
\centering
\includegraphics{source/spreadsheet.png}
\caption{Spreadsheet screenshot - dataset size: 10000}
\end{figure}


# Conclusion

Neo4j was the slowest NoSQL DBMS between all the tested ones. This is because Neo4j is not designed as a big data storage or as a distributed DBMS with emphasis on query speed. Neo4j is instead designed as a powerful schema-less graph database, which is extremely useful in situations where complex relationships without a fixed design need to be managed.

Benchmarking 100000 patients was not feasible on the machine used for the tests, due to extremely long Neo4j data loading times with a huge number of floating point values.

`query1` is the fastest for large dataset sizes, as the filter used to match patients is very strict. `query2`, due to the complex filtering rules, is always the slowest because it's necessary to iterate over the `measurement` nodes connected to the `patient` nodes.



## Links

The project is available on GitHub: [https://github.com/SuperV1234/db2](https://github.com/SuperV1234/db2).


# References
