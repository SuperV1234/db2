

### Query benchmarker

The second Python script, which runs the queries, benchmarks them and produces the graphs thanks to **matplotlib**, has a very straightforward implementation:

```python
# Create a `master` 
m = master(make_connection("neo4j", "admin"))

# Benchmark queries
bench_query('query0', '''
    MATCH (n:patient)
    RETURN n''')

bench_query('query1', '''
    MATCH (n:patient)
    WHERE n.n = "SIVV33W0"
    RETURN n''')

bench_query('query2', '''
    MATCH (p:patient)-[r:measure]->(m:measurement)
    WITH p, m, count(m) as relcount
    WHERE p.lwalk_td < 5000 AND p.w <> 5000 AND relcount > 4
    RETURN p''')
```

The `bench_query` functions is implemented as follows:

```python
# Executes `q`, timing it and outputting results
def bench_query(lbl, q):
    # Perform queries and time them
    # Write results as newline-separated values
    for i in range(0, 30):
        start_timer()
        m.exec_query(q)
        print(end_timer())
```

The `start_timer` and `end_timer` functions make use of the `time.perf_counter()` Python high-precision timer in order to retrieve the execution time of every single query:

```python
# Benchmark utilities
t0 = []
def start_timer():
    global t0
    t0.append(time.perf_counter())

def end_timer():
    global t0
    val = time.perf_counter() - t0.pop()
    return val
```

### Plotting script

After having generated all dataset results in text files, where every query iteration execution time is written to a different line, the plotting script will take care of reading the files and producing histogram plots.

Given a list of dataset output paths `datasets` and a count of measurements per output, the `create_plot` is called for every query:

```python
create_plot("query 0", datasets, 0, count, "plots/query0.png")
create_plot("query 1", datasets, 30, count, "plots/query1.png")
create_plot("query 2", datasets, 60, count, "plots/query2.png")
```

Its implementation is as follows:

```python
def create_plot(plot_title, datasets, offset, count, output_path):
    # Iterate over the dataset benchmark outputs
    for dataset_path in datasets:
        # Get the first query execution time and 
        # the remaining queries' average time
        first,avg = first_and_avg(dataset_path, offset, count)

        # Create two bars using pyplot
        b_first = plt.bar(x, first, 0.5, color='b')
        b_avg = plt.bar(x, avg, 0.5, color='r')
    
    # Plot to file
    plt.savefig(output_path)
    plt.clf()
```

The `first_and_avg` function simply loads the dataset timing results in memory and returns a tuple containing the first query time and the average time of the remaining queries.

### Automation script

In order to automate the whole benchmarking process, a simple **bash** script was implemented to load all datasets and execute the queries on them:

```bash
# Create `results` folder if required
mkdir -p results

# Dataset N array
VALUES=(10 100 1000 10000 100000)

# Load dataset chunk N array
CHUNKS=(1 1 1 5 10)

# Next chunk value index
ICHUNK=0

for i in "${VALUES[@]}"
do
    # Dataset path
    DS="../../dataset_lokomat/output/ds${i}.json"

    # Output graph path
    OF="./results/r${i}.png"

    # Load dataset
    python3 -O ./load_dataset.py "${DS}" "${CHUNKS[$ICHUNK]}"

    # Increment index for next chunk
    ((ICHUNK++))

    # Run queries and create plots
    python3 -O ./queries.py "${OF}"
done

# Create plots
python3 -O ./make_plots.py
```

The script, in short, simply runs the previously described Python scripts for every randomly-generated dataset, automatically passing the dataset path and a reasonable chunk value in every execution. When the datasets have been processed, the `make_plots` script is finally called to produce the histogram images.


# Conclusion

The results of the queries are provided as histograms. Every plot image represents a single query, over all datasets. Two bars are plotted per dataset size: the first bar represents the **execution time of the first query**, the second bar represents the **average execution time of the remaining queries**.

The **X axis** represents the size of the datasets.

The **Y axis** represents the execution time, in milliseconds.

## Result graphs

\begin{figure}[H]
\centering
\includegraphics{source/query0.png}
\caption{Benchmark results: query 0}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{source/query1.png}
\caption{Benchmark results: query 1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics{source/query2.png}
\caption{Benchmark results: query 2}
\end{figure}

Benchmarking 100000 patients was not feasible on the machine used for the other tests, due to extremely long Neo4j data loading times.

`query1` is the fastest for large dataset sizes, as the filter used to match patients is very strict. `query2`, due to the complex filtering rules, is always the slowest because it's necessary to iterate over the `measurement` nodes connected to the `patient` nodes.


## Links

The project is available on GitHub: [https://github.com/SuperV1234/db2](https://github.com/SuperV1234/db2).