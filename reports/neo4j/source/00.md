


---
references:
- type: article-journal
  id: wikipedianeo4j
  title: Wikipedia - Neo4j
  URL: https://en.wikipedia.org/wiki/Neo4j
---



\begin{titlepage}
    \begin{center}

        \vspace{1.5cm}
        
        \huge
        NoSQL Hospital Information System (HIS) benchmark report - Neo4j
        
        \vspace{1.5cm}
        
        \Large
        Vittorio Romeo (mat. 444962) \\
        Sergio Zavettieri (mat. 447265)

        \vspace{1.5cm}

        Professor: \\
        Antonio Celesti

        \vfill
        
        \vspace{0.8cm}

        \normalsize
        UniversitÃ  degli Studi di Messina \\
        Dipartimento di Scienze Matematiche e Informatiche, \\Scienze Fisiche e Scienze della Terra

    \end{center}
\end{titlepage}

\tableofcontents



# Introduction

As part of the Databases II course, a group project consisting in the development of a benchmark for multiple NoSQL DBMSs was requested. All the DBMSs have to perform and time the same queries, in order to fairly compare their performance on a randomly-generated HIS dataset *(following a schema provided by the professor)*.

The group benchmarked the performance of the following NoSQL DBMSs:

* **Neo4j**.

* **Cassandra**.

* **MongoDB**.

* **HBase**.

This report covers the **Neo4j** benchmark development and implementation.




# Reference environment

## Client request

The client requests an application that benchmarks the Neo4j NoSQL DBMS with randomly-generated data, in order to understand its performance with a specific schema *(used for an Hospital Information System)* and to see how the system's efficiency scales with an increasing data amount.

The desired schema is shown below:

\dot(source/schema)(Client dataset schema)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
digraph
{
    rankdir=LR

    patient [shape="rectangle"]
    measurement [shape="rectangle"]
    has [shape="diamond"]

    patient -> has [label="0..N" dir="none"]
    has -> measurement [label="1..1" dir="none"]
}
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The requested application must import several randomly-generated datasets, and benchmark three different queries:

1. Select all patients.

2. Select all patients matching a specific condition.

3. Select all patients matching a specific condition with at least 5 measurements.

The generated datasets must be five and contain the following element amounts: `10`, `100`, `1000`, `10000`, `100000`.

After running the queries, the client requests histogram plots of the results, by repeating the same query `50` times.


# Tools and development environment

The application was implemented using **Python 3.5.1** and the [**Neo4j** NoSQL DBMS](http://neo4j.com/).

**Neo4j** is a graph database management system developed by Neo Technology, Inc. Described by its developers as an ACID-compliant transactional database with native graph storage and processing, Neo4j is the most popular graph database according to db-engines.com [@wikipedianeo4j].

The following libraries were used as dependencies:

* [**neo4j-rest-client**](http://neo4j-rest-client.readthedocs.io/): a Python interface to interact with Neo4j.

* [**matplotlib**](http://matplotlib.org/): a Python library to easily create plots and graphs.

# Design 


The dataset was modeled in Neo4j as follows:

* **Patients** are nodes labeled `patient`.

* **Measurements** are node labeled `measurement`.

* Patients are related to $0..N$ measurements via the `measure` relationship.

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{source/screen.png}
\caption{Neo4j web interface screenshot}
\end{figure}



Three different Python scripts were developed for the project:

* `load_dataset.py`, which, given a JSON dataset, loads it into the current Neo4j instance.

* `queries.py`, which runs the queries on the current Neo4j instance and outputs the query execution times to text files.

* `make_plots.py`, which reads the data outputted by the previous script and uses `matplotlib` to create histogram plots.

An helper bash script, `run_benchmarks.sh`, was developed to automate the benchmarking process.




# Implementation

## Dataset loader

The dataset loader is conceptually very simple. The dataset path and a **chunk value** *(which represents the number of insertions to execute in batch)* are taken as command-line parameters. 

A `master` class was created to wrap the Neo4j connection and functions to run the queries:

```python
# Class containing an open neo4j connection 
# and functions to manage the data
class master:
    # Defines a label `l` and stores it in the 
    # labels list
    def define_label(self, l):
        self.labels[l] = self.db.labels.create(l)

    # Constructor
    # Given an open connection, stores the connection 
    # in `master`
    # Defines labels for every entity
    def __init__(self, db):
        self.db = db
        self.labels = {}

    # Completely clears the database
    def delete_everything(self):
        q = 'MATCH (n) DETACH DELETE n'

        self.db.query(q)

        self.define_label("patient")
        self.define_label("measurement")

        tx = self.db.transaction(for_query=True)
        tx.append("CREATE INDEX ON :patient(id)")
        tx.execute()
        tx.commit()

    # Executes the string `x` as a query
    def do_query(self, x):
        tx = self.db.transaction(for_query=True)
        tx.append(x)
        tx.execute()
        tx.commit()
```

The rest of the application deals with dataset loading and query generation, using an efficient string concatenation method thanks to the `StringBuilder` class:

```python
# Helper class for efficient string concatenation
class StringBuilder(object):
    def __init__(self):
        self._stringio = io.StringIO()

    def __str__(self):
        return self._stringio.getvalue()

    def append(self, *objects, sep=' ', end=''):
        print(*objects, sep=sep, end=end, file=self._stringio)
```

The `main` function is as follows:

```python
# Create a `master` and clear the database
m = master(make_connection("neo4j", "admin"))
m.delete_everything()

# Read dataset path from command line arguments
dataset_path = sys.argv[1]

# Read how many queries to batch per transaction
chunk_size = int(sys.argv[2])

# Read the dataset file as json
ds_patients = json.loads(open(dataset_path, "r").read())

# Index used to generate unique node names
idx = 0

# Execute all insertions
for i in range(0, len(ds_patients), chunk_size):
    q = StringBuilder()

    # Iterate patients in chunks
    for p in ds_patients[i:i + chunk_size]:
        # Stringify `idx`
        sidx = str(idx)

        # Generate patient node creation query
        q.append("CREATE (n")
        q.append(sidx)
        q.append(":patient {")
        q.append(make_patient_dict(p))
        q.append("})\n")

        # Generate measurement queries, which build relationships
        for s in p["step_datas"]:
            q.append("CREATE (n")
            q.append(sidx)
            q.append(")-[:measure]->(:measurement {")
            q.append(make_measurement_dict(s))
            q.append("})\n")

        # Increment next unique node id
        idx += 1

    m.do_query(str(q))
```

The `make_patient_dict` and `make_measurement_dict` functions efficiently build **Cypher** strings for the insertion of multiple parameters.
